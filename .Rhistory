z[2]
w[2]
e<-1:4
r<-2
e+r
g<-e+r
class(g)
h<-c(1,2,3,4,5,6)
h
h[h<6]==0
h[h<6]<-0
h
h[h%in%1:5]<-0
h
h<-c(1,2,3,4,5,6)
h[h%in%1:5]<-0
h
h<-c(1,2,3,4,5,6)
h[h<=5]<-0
h
airquality
airlift[1:2, ]
airquality[1:2, ]
nrows(airquality)
nrows[airquality]
nrow[airquality]
nrow(airquality)
tail(airquality)
tail<-tail(airquality)
tail
airquality[nrow(airquality):nrow(airquality)-1, ]
airquality[nrow(airquality):nrow(airquality)-2, ]
airquality[nrow(airquality):(nrow(airquality)-1), ]
airquality[nrow(airquality)nrow(airquality)(nrow(airquality)-1), ]
airquality[(nrow(airquality)-1):nrow(airquality), ]
airquality[1,47]
airquality[47,1]
airquality[47,airquality$ozone]
airquality[47,$ozone]
airquality$ozone
airquality$Ozone
n<-airquality$ozone
is.na(n)
g<-complete.cases(n)
g<-complete.cases(airquality$Ozone)
q
g
is.na(airquality$Ozone])
is.na(airquality$Ozone)
n[g]
n[good, ]
n[g, ]
n[g]
g
n<-c(airquality$Ozone)
n
n[g]
m<-n[g]
length(m)
m<-n[!g]
length(m)
mean(m)
m
mean(!m)
n[g]
mean(n[g])
i<-airquality[((airquality$Ozone>31) && (airquality$Temp>90)), ]
i
i<-airquality[(airquality$Ozone>31), ]
i
j<-airquality[airquality$Temp>90]
j<-airquality[airquality$Temp>90, ]
j
k<-complete.cases(i,j)
k <- merge(i,j)
k
mean(k$Solar.R)
airquality[airquality$Month=6, ]
airquality[airquality$Month==6, ]
h<-airquality[airquality$Month==6, ]
mean(h$Temp)
h<-airquality[airquality$Month==5, ]
h
max(h$Ozone)
v<-is.na(h$Ozone)
v
h[!v, ]
h<-h[!v, ]
h
max(h$Ozone)
q()
x=10
y=if(x>3){
}
y
y=if(x>3){
10
}
else{
x=10
y=if(x>3){10}else{0}
y
x=2
y
y=if(x>3){10}else{0}
y
y=if(x>3){10}else0
y=if(x>3){10}else 0
y
+x<-c("a","b","c","d")
x<-c("a","b","c","d")
for(i in 1:4)
{print(x[i])}
for(i in seq_along(x))
{print(x[i])}
xy<-c("a","b","f","d","g")
for(i in seq_along(y))
{print(y[i])}
y<-c("a","b","f","d","g")
for(i in seq_along(y))
{print(y[i])}
for(letter in x)
{print x}
{print(letter)}
for(letter in x)
{print(letter)}
for(letter in y)
{print(letter)}
for(i in 1:4) print(x[i])
for(i in seq_len(x))
{print(x[i])}
{print(x[i])}
for(i in seq_len(x))
{print(i)}
x<-matrix(1:6,2,3)
for(i in seq_len(nrow(x)))
{for(j in seq_len(ncol(x)))
{print(x[i,j])}
}
{print(x[j,i])}
for(i in seq_len(nrow(x)))
{for(j in seq_len(ncol(x)))
{print(x[j,i])}
}
q()
View(mm)
iris
airquality
columnmean<-function(x){
n<-ncol(x)
colmean<-numeric(n)
for(i in 1:n){
colmean[i]=mean(x[,i])
}
colmean
}
columnmean(iris)
columnmean(airquality)
columnmean<-function(x){
n<-ncol(x)
colmean<-numeric(n)
for(i in 1:n){
colmean[i]=mean(x[,i],na.rm=TRUE)
}
colmean
}
columnmean(airquality)
columnmean<-function(x,y=TRUE){
n<-ncol(x)
colmean<-numeric(n)
for(i in 1:n){
colmean[i]=mean(x[,i],na.rm=y)
}
colmean
}
columnmean(airquality)
columnmean(airquality,FALSE)
columnmean(iris)
columnmean(iris,FALSE)
search()
source("http://bioconductor.org/bioc  lite.R")
source("http://bioconductor.org/biocLite.R")
biocLite("rhdf5")
library(rhdf5)
x=h5createFile("example.h5")
x
x=GET("https://accounts.google.com/signin/v2/identifier?uilel=3&service=youtube&passive=true&continue=https%3A%2F%2Fwww.youtube.com%2Fsignin%3Faction_handle_signin%3Dtrue%26app%3Ddesktop%26next%3D%252F%26hl%3Den&hl=en&flowName=GlifWebSignIn&flowEntry=ServiceLogin",authenticate("palashcool10@gmail.com"))
library(httr)
x=GET("https://accounts.google.com/signin/v2/identifier?uilel=3&service=youtube&passive=true&continue=https%3A%2F%2Fwww.youtube.com%2Fsignin%3Faction_handle_signin%3Dtrue%26app%3Ddesktop%26next%3D%252F%26hl%3Den&hl=en&flowName=GlifWebSignIn&flowEntry=ServiceLogin",authenticate("palashcool10@gmail.com"))
x=GET("https://accounts.google.com/signin/v2/identifier?uilel=3&service=youtube&passive=true&continue=https%3A%2F%2Fwww.youtube.com%2Fsignin%3Faction_handle_signin%3Dtrue%26app%3Ddesktop%26next%3D%252F%26hl%3Den&hl=en&flowName=GlifWebSignIn&flowEntry=ServiceLogin",authenticate("palashcool10@gmail.com","netprotectorantivirussssssssssss"))
x
names(x)
google=handle("https://www.google.com/")
x=GET(handle=google,path="/")
x
x=GET(handle=google,path="search")
names(x)
source('C:/Users/Windows/Desktop/twitter_analysis.R')
install.packages("httr")
install.packages("syuzhet")
install.packages("twitteR")
install.packages("RCurl")
install.packages("httr")
install.packages("syuzhet")
yes
install.packages("wordcloud")
install.packages("tm")
library(twitteR)
library(RCurl)
library(httr)
library(tm)
library(wordcloud)
library(syuzhet)
consumer_key = "eAWXlQ9LFSYyQ7gRHSVugwg3U"
consumer_secret = "TtdAVy8RNkCuUAAPmtU9ejm863MBOZY3vVZ2i5YNjscYjtYgIc"
access_token = "877210847445004288-RuWS9UzPRl1j7eN8p4NhzO2c4QY41mM"
access_secret ="7BCHEBk4oowSQeHIr1Datqh27QA6dhFAOosLgiu7V50LV"
setup_twitter_oauth(consumer_key,consumer_secret,access_token, access_secret)
prp(tweetCART,extra=2)
consumer_key = "eAWXlQ9LFSYyQ7gRHSVugwg3U"
setup_twitter_oauth(consumer_key,consumer_secret,access_token, access_secret)
tweets = searchTwitter("#MondayMotivation", n = 200, lang = "en")
tweets.df$text=gsub("&amp", "", tweets.df$text)
tweets.df$text = gsub("&amp", "", tweets.df$text)
tweets.df$text = gsub("(RT|via)((?:\\b\\W*@\\w+)+)", "", tweets.df$text)
tweets.df$text = gsub("@\\w+", "", tweets.df$text)
tweets.df$text = gsub("[[:punct:]]", "", tweets.df$text)
tweets.df$text = gsub("[[:digit:]]", "", tweets.df$text)
tweets.df$text = gsub("http\\w+", "", tweets.df$text)
tweets.df$text = gsub("[ \t]{2,}", "", tweets.df$text)
tweets.df$text = gsub("^\\s+|\\s+$", "", tweets.df$text)
library(httpuv)
library(httr)
library(jsonlite)
library(httpuv)
oauth_endpoints("github")
myapp <- oauth_app("quiz2",key ="3c665a9b06f12b90e41b",secret="347e49d40d4d5fb6d3c2f75e47fd75d45b9487cb")
github_token <- oauth2.0_token(oauth_endpoints("github"), myapp)
install.packages("httpuv")
github_token <- oauth2.0_token(oauth_endpoints("github"), myapp)
req <- GET("https://api.github.com/users/jtleek/repos",
config(token = github_token))
step_for_status(req)
stop_for_status(req)
x=content(req)
json=fromJSON(toJSON(x))
subset(json,name==datasharing,select=c(created_at))
subset(json,name=="datasharing",select=c(created_at))
install.package("sqldf")
install.packages("sqldf")
library(sqldf)
getwd()
setwd("G:\new start\10 courses\getting and cleaning data\week2")
setwd("\G:\new start\10 courses\getting and cleaning data\week2")
setwd("/G:/new start/10 courses/getting and cleaning data/week2")
setwd("G:/new start/10 courses/getting and cleaning data/week2")
setwd(choose.dir())
pnorm(1.15)
install.packages(tm)
install.packages("tm")
setRepositories()
1 2 3 4 5 6 7 8 9 10
1
setRepositories(ind = c(1:6, 8))
install.packages("tm")
install.packages("shiny")
install.packages("data.table")
install.packages("shiny")
options(java.parameters = "-Xmx8000m")
library(tm)
library(stylo)
library(data.table)
library(RWeka)
library(dplyr)
library(stringr)
con <- file("en_US.blogs.txt")
blogs <- readLines(con)
getwd()
setwd(/Users/Windows/Desktop/New folder/DataScienceCapstone-master)
setwd(Users/Windows/Desktop/New folder/DataScienceCapstone-master)
setwd("/Users/Windows/Desktop/New folder/DataScienceCapstone-master")
options(java.parameters = "-Xmx8000m")
library(tm)
library(stylo)
library(data.table)
library(RWeka)
library(dplyr)
library(stringr)
con <- file("en_US.blogs.txt")
blogs <- readLines(con)
close(con)
con <- file("en_US.news.txt", open="rb")
news <- readLines(con)
close(con)
con <- file("en_US.twitter.txt")
twitter <- readLines(con)
close(con)
rm(con)
source('functions/sampleData.R')
# These percentages were chosen to give us roughly 80,000 thousands samples in each variable
# This action was performed due to memory limitations
set.seed(1738)
blogs_Sample <- sampleData(blogs,percent = 0.1)
news_Sample <- sampleData(news,percent = 0.089)
twitter_Sample <- sampleData(twitter,percent = 0.035)
save(blogs_Sample, news_Sample, twitter_Sample, file = "mySampledData.RData")
rm(list=ls())
load("mySampledData.RData")
source("functions/cleanUserInput.R")
# Clean data, save file
blogsClean <- cleanUserInput(blogs_Sample)
newsClean <- cleanUserInput(news_Sample)
twitterClean <- cleanUserInput(twitter_Sample)
save(blogsClean, newsClean, twitterClean, file = "mycleanData.RData")
rm(list = ls())
load("mycleanData.RData")
# Create word list from cleaned data, save file
# These three files will be used to create a unigram list
dataBlogWords <- txt.to.words(blogsClean)
dataNewsWords <- txt.to.words(newsClean)
dataTwitterWords <- txt.to.words(twitterClean)
save(dataBlogWords, dataNewsWords, dataTwitterWords, file = "mytxt2words.RData")
rm(dataBlogWords, dataNewsWords, dataTwitterWords)
# Convert cleaned data into a corpus
blogsCorpus <- VCorpus(VectorSource(blogsClean))
newsCorpus <- VCorpus(VectorSource(newsClean))
twitterCorpus <- VCorpus(VectorSource(twitterClean))
save(blogsCorpus, newsCorpus, twitterCorpus, file = "my_corpus234gram.RData")
rm(blogsClean, newsClean, twitterClean)
# Create word list from corpus variables, these variables will be used to create 2,3,4 gram tables
dataBlogWords <- txt.to.words(blogsCorpus)
dataNewsWords <- txt.to.words(newsCorpus)
dataTwitterWords <- txt.to.words(twitterCorpus)
save(dataBlogWords, dataNewsWords, dataTwitterWords, file = "mytxt2wordsCorpus.RData")
rm(blogsCorpus, newsCorpus, twitterCorpus)
source("functions/getNGrams.R")
# Create raw 2gram variables
raw2gramB <- getNGrams(dataBlogWords, 2)
raw2gramN <- getNGrams(dataNewsWords, 2)
raw2gramT <- getNGrams(dataTwitterWords, 2)
save(raw2gramT, raw2gramN, raw2gramB, file = "myraw2gram.RData")
rm(raw2gramT, raw2gramN, raw2gramB)
# Create raw 3gram variables
raw3gramB <- getNGrams(dataBlogWords, 3)
raw3gramN <- getNGrams(dataNewsWords, 3)
raw3gramT <- getNGrams(dataTwitterWords, 3)
save(raw3gramT, raw3gramN, raw3gramB, file ="myraw3gram.RData")
rm(raw3gramT, raw3gramN, raw3gramB)
# Create raw 4gram variables
load("mytxt2wordsCorpus.RData")
rm(dataBlogWords, dataNewsWords)
raw4gramT <- getNGrams(dataTwitterWords, 4)
rm(dataTwitterWords)
save(raw4gramT, file = "myraw4gram.RData")
rm(raw4gramT)
load("mytxt2wordsCorpus.RData")
rm(dataTwitterWords, dataNewsWords)
raw4gramB <- getNGrams(dataBlogWords, 4)
rm(dataBlogWords)
load("myraw4gram.RData")
save(raw4gramB, raw4gramT, file = "myraw4gram.RData")
rm(raw4gramB, raw4gramT)
load("mytxt2wordsCorpus.RData")
rm(dataTwitterWords, dataBlogWords)
raw4gramN <- getNGrams(dataNewsWords, 4)
rm(dataNewsWords)
load("myraw4gram.RData")
save(raw4gramT, raw4gramN, raw4gramB, file = "myraw4gram.RData")
rm(list=ls())
# Create tables for 2,3,4 gram word counts
load("myraw2gram.RData")
table2gram <- table(c(raw2gramB, raw2gramN, raw2gramT))
save(table2gram, file = "myraw2gramTable.RData")
rm(list=ls())
load("myraw3gram.RData")
table3gram <- table(c(raw3gramB, raw3gramN, raw3gramT))
save(table3gram, file = "myraw3gramTable.RData")
rm(list=ls())
load("myraw4gram.RData")
table4gram <- table(c(raw4gramB, raw4gramN, raw4gramT))
save(table4gram, file = "myraw4gramTable.RData")
rm(list=ls())
load("mytxt2words.RData")
raw1gram <- table(c(dataBlogWords, dataNewsWords, dataTwitterWords))
wf1gram <- data.frame(word=names(raw1gram), freq=as.numeric(raw1gram), stringsAsFactors = FALSE)
wf1gram <- filter(wf1gram, freq != 1)
wf1gram <- arrange(wf1gram, desc(freq))
save(wf1gram, file = "myraw1gramTableFiltered.RData")
rm(list = ls())
load("myraw2gramTable.RData")
wf2gram <- data.frame(word=names(table2gram), freq=as.numeric(table2gram), stringsAsFactors = FALSE)
wf2gram <- filter(wf2gram, freq != 1)
wf2gram <- arrange(wf2gram, desc(freq))
save(wf2gram, file = "myraw2gramTableFiltered.RData")
rm(list=ls())
load("myraw3gramTable.RData")
wf3gram <- data.frame(word=names(table3gram), freq=as.numeric(table3gram), stringsAsFactors = FALSE)
wf3gram <- filter(wf3gram, freq != 1)
wf3gram <- arrange(wf3gram, desc(freq))
save(wf3gram, file = "myraw3gramTableFiltered.RData")
rm(list=ls())
load("myraw4gramTable.RData")
wf4gram <- data.frame(word=names(table4gram), freq=as.numeric(table4gram), stringsAsFactors = FALSE)
wf4gram <- filter(wf4gram, freq != 1)
wf4gram <- arrange(wf4gram, desc(freq))
save(wf4gram, file = "myraw4gramTableFiltered.RData")
rm(list=ls())
load("myraw1gramTableFiltered.RData")
load("myraw2gramTableFiltered.RData")
load("myraw3gramTableFiltered.RData")
load("myraw4gramTableFiltered.RData")
gram1 <- c(1,nrow(wf1gram))
gram2 <- c(gram1[2]+1, gram1[2]+nrow(wf2gram))
gram3 <- c(gram2[2]+1, gram2[2]+nrow(wf3gram))
gram4 <- c(gram3[2]+1, gram3[2]+nrow(wf4gram))
save(gram1, gram2, gram3, gram4, file = "divideNGram.RData")
rm(gram1, gram2, gram3, gram4)
# Concatenate tables, write .txt file
tableNGram <- rbind(wf1gram, wf2gram, wf3gram, wf4gram)
rm(wf1gram, wf2gram, wf3gram, wf4gram)
save(tableNGram, file = "mywriteTableFINAL.RData")
write.table(tableNGram, file = "NGramSortedFinal.txt", sep = "\t", row.names = FALSE)
getPredWord <- function(phraseIn, wordsOut = 5, flag = TRUE) {
# lastWord <- NA
userWords <- txt.to.words(cleanUserInput(phraseIn, flag))
numWords <- length(userWords)
# If user input is blank space or is filtered out, return common unigrams
if (numWords==0){
lastWord <- as.data.frame(as.character(data1gram$word[1:wordsOut]))
return(lastWord)
}
# Cuts user input down to the last 3 words
if (numWords > 3) {
numWords <- 3
userWords <- tail(userWords, numWords)
}
# This for loop searches for matches, removes common results if necessary
matchLen <- NULL
matchList <- NULL
for (i in numWords:1) {
tempResults <- NULL
tempResults <- getNextWord(tail(userWords, i))
if (is.na(tempResults[1])) {
matchLen <- c(matchLen, 0)
} else {
logicRemove <- tempResults %in% matchList
matchList <- c(matchList, tempResults[!logicRemove])
matchLen <- c(matchLen, length(tempResults[!logicRemove]))
rm(logicRemove, tempResults)
}
if (sum(matchLen) > wordsOut) {break}
}
# If user phrase is non english or fake words return unigram list
if (sum(matchLen)==0) {
lastWord <- as.data.frame(as.character(data1gram$word[1:wordsOut]))
return(lastWord)
}
# These 3 lines remove zeros from the term matchLen
# revInd is used to trim the user input for the getMatchProb function
revInd <- numWords:1
revInd <- revInd[!(matchLen %in% 0)]
matchLen <- matchLen[!(matchLen %in% 0)]
# Calculate stupid backoff adjustment
stepDown <- NULL
for (i in 1:length(matchLen) ) {
if(matchLen[i]!=0){
if (i==1){
stepDown <- c(stepDown, rep(log(1), matchLen[i]) )
} else {
stepDown <- c(stepDown, rep(log(0.4^(i-1)), matchLen[i]))
}
} else {
stepDown <- c(stepDown, NULL)
}
}
# Add comments
if (length(matchList)>20) {
matchList <- matchList[1:20]
stepDown <- stepDown[1:20]
}
# Calculate log probability
prob <- NULL
for (i in 1:length(unique(stepDown))) {
temp <- NULL
temp <- stepDown %in% unique(stepDown)[i]
prob <- c(prob, getMatchProb( matchList[temp], tail(userWords, revInd[i]) ))
}
# Create data frame of results, sort results
bestGuess <- data.frame( word=matchList, logProb=as.numeric(prob+stepDown), stringsAsFactors = FALSE )
bestGuess <- arrange(bestGuess, desc(logProb))
return(bestGuess[1:wordsOut,])
}
shiny::runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
